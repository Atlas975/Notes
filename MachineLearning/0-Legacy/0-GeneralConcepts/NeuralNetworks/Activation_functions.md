# Activation functions
[[NeuralNetwork_intuition]]
![[Pasted image 20220221221802.png|450|450]]


a linear hidden layer is useless as it cant help at all in computing more interesting functions, a linear function might be used in the output layer however
[[ReLU_function]]
## Activation function derivatives
![[Pasted image 20220303160437.png|450|450]]